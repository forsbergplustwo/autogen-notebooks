{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "tags: [\"group chat\", \"orchestration\", \"RAG\"]\n",
    "description: |\n",
    "    Implement and manage a multi-agent chat system using AutoGen, where AI assistants retrieve information, generate code, and interact collaboratively to solve complex tasks, especially in areas not covered by their training data.\n",
    "-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Chat with Retrieval Augmented Generation\n",
    "\n",
    "AutoGen supports conversable agents powered by LLMs, tools, or humans, performing tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install autogen\n",
    "%pip install pyautogen\n",
    "%pip install pyautogen[autobuild]\n",
    "%pip install pyautogen[retrievechat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import autogen\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from autogen import AssistantAgent, GroupChat, GroupChatManager, UserProxyAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "CACHE_SEED = 98\n",
    "\n",
    "config_file_or_env = \"./OAI_CONFIG_LIST.json\"\n",
    "config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={\"model\": [\"gpt-4-1106-preview\", \"gpt-3.5-turbo\"]})\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 60,\n",
    "    \"cache_seed\": CACHE_SEED,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "# RetrieveUserProxyAgent is a user proxy agent that can retrieve content from a database.\n",
    "autogen.agentchat.contrib.retrieve_user_proxy_agent.PROMPT_QA = \"\"\"\n",
    "Here is the research summary for the provided company app information:\n",
    "\n",
    "{input_context}\n",
    "\"\"\"\n",
    "\n",
    "ceo = UserProxyAgent(\n",
    "    name=\"CEO\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"CEO of the company. Gives tasks to the team, and expects them to be completed.\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "ceo_aid = RetrieveUserProxyAgent(\n",
    "    name=\"Researcher\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"Assistant who has extra content retrieval power for finding company app information.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=2,\n",
    "    llm_config=llm_config,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": \"https://gist.githubusercontent.com/forsbergplustwo/835f5134190902f7cca3d943b3a3b9db/raw/5c3a6460cfbe4f4e890ca40228db115aace3242c/digital_downloads_pro.txt\",\n",
    "        \"chunk_token_size\": 1000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"groupchat\",\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    "    code_execution_config=False,  # we don't want to execute code in this case.\n",
    ")\n",
    "\n",
    "ceo_researcher = RetrieveAssistantAgent(\n",
    "    name=\"researcher\",\n",
    "    system_message=\"You are a helpful research assistant. Use the [retrieve_company_app_info] function to get company and app information for the user task. Reply with the full text and initial task description.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=2,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# Load agent team agents from a config file.\n",
    "builder = AgentBuilder(config_file_or_env=config_file_or_env)\n",
    "agent_list, _agent_configs = builder.load(\n",
    "    \"./marketing_agents.json\",\n",
    ")\n",
    "\n",
    "def _reset_agents():\n",
    "    ceo.reset()\n",
    "    ceo_aid.reset()\n",
    "    ceo_researcher.reset()\n",
    "    # Actual agents.\n",
    "    for agent in agent_list:\n",
    "        agent.reset()\n",
    "\n",
    "\n",
    "def call_rag_chat(message=\"\"):\n",
    "    _reset_agents()\n",
    "\n",
    "    # Register the function for the CEO assistant agents.\n",
    "    @ceo_aid.register_for_execution()\n",
    "    @ceo_researcher.register_for_llm(description=\"Retreive company and app information.\")\n",
    "    def retrieve_company_app_info(\n",
    "        message: Annotated[str, \"The original task description.\"],\n",
    "        app_name: Annotated[str, \"App name to lookup\"] = \"\",\n",
    "        n_results: Annotated[int, \"Number of results to retrieve.\"] = 1,\n",
    "    ) -> str:\n",
    "        ceo_aid.n_results = n_results\n",
    "        update_context_case1, update_context_case2 = ceo_aid._check_update_context(message)\n",
    "        if (update_context_case1 or update_context_case2) and ceo_aid.update_context:\n",
    "            ceo_aid.problem = message if not hasattr(ceo_aid, \"problem\") else ceo_aid.problem\n",
    "            _, ret_msg = ceo_aid._generate_retrieve_user_reply(message)\n",
    "        else:\n",
    "            ret_msg = ceo_aid.generate_init_message(message, n_results=n_results)\n",
    "        return ret_msg if ret_msg else message\n",
    "\n",
    "\n",
    "    for agent in [ceo_researcher, *agent_list]:\n",
    "        agent.llm_config = llm_config\n",
    "        agent.human_input_mode = \"NEVER\"\n",
    "        agent.is_termination_msg = termination_msg\n",
    "\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[ceo, ceo_researcher, ceo_aid, *agent_list],\n",
    "        messages=[],\n",
    "        max_round=12,\n",
    "        speaker_selection_method=\"auto\",\n",
    "        allow_repeat_speaker=True,\n",
    "    )\n",
    "\n",
    "    manager_llm_config = llm_config.copy()\n",
    "    # manager_llm_config.pop(\"functions\")\n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=groupchat,\n",
    "        llm_config=manager_llm_config,\n",
    "\n",
    "    )\n",
    "\n",
    "    # Start chatting with the boss as this is the user proxy agent.\n",
    "    ceo.initiate_chat(\n",
    "        manager\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blog post for a new feature for our Digital Downloads Pro app. The new feature is: Set expiration dates on License Keys.\n",
    "call_rag_chat(message=\"\")\n",
    "\n",
    "# rag_chat()\n",
    "# type exit to terminate the chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
